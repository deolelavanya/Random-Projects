{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4a0949",
   "metadata": {
    "id": "bf4a0949"
   },
   "source": [
    "# IMDb Knowledge Distillation Notebook\n",
    "This notebook fine-tunes a BERT teacher and trains a small student model via knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iX1uhqCBN_yW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iX1uhqCBN_yW",
    "outputId": "0816520c-1eea-4f34-f24a-62033deb687b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "HZHDeHmaPOHP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZHDeHmaPOHP",
    "outputId": "d6414fdb-d895-4e79-ae9a-b65be2b724c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce5a30d",
   "metadata": {
    "id": "dce5a30d"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports & warnings (updated)\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The secret `HF_TOKEN` does not exist in your Colab secrets\"\n",
    ")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW           # ← moved here\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_scheduler                   # ← removed AdamW from here\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18938551",
   "metadata": {
    "id": "18938551"
   },
   "outputs": [],
   "source": [
    "def load_data(num_train=20000, num_test=5000, max_length=256, batch_size=16):\n",
    "    raw = load_dataset(\"imdb\")\n",
    "    raw_train = raw[\"train\"].shuffle(seed=42).select(range(num_train))\n",
    "    raw_test  = raw[\"test\"].shuffle(seed=42).select(range(num_test))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    def preprocess(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    train_ds = raw_train.map(preprocess, batched=True)\n",
    "    test_ds  = raw_test.map(preprocess, batched=True)\n",
    "\n",
    "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    test_ds.set_format(type=\"torch\",  columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
    "    return train_loader, test_loader, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c40580",
   "metadata": {
    "id": "b5c40580"
   },
   "outputs": [],
   "source": [
    "def train_teacher(model, train_loader, device, lr=2e-5, epochs=1):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = get_scheduler(\n",
    "        \"linear\", optimizer=optimizer,\n",
    "        num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    model.to(device).train()\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(train_loader, desc=f\"[Teacher] Epoch {epoch+1}\", leave=False)\n",
    "        for batch in loop:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            loss = model(**batch).loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "628f9d0c",
   "metadata": {
    "id": "628f9d0c"
   },
   "outputs": [],
   "source": [
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=128, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=4,\n",
    "            dim_feedforward=hidden_dim*2, dropout=0.1\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed(input_ids)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        mask = ~attention_mask.bool()\n",
    "        x = self.transformer(x, src_key_padding_mask=mask)\n",
    "        x = x.mean(dim=0)\n",
    "        logits = self.classifier(x)  # Assign output to logits\n",
    "        return logits # Return logits instead of just the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1817935d",
   "metadata": {
    "id": "1817935d"
   },
   "outputs": [],
   "source": [
    "def train_student(student, teacher, train_loader, device,\n",
    "                  lr=5e-4, epochs=1, alpha=0.5, temperature=2.0):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    optimizer = AdamW(student.parameters(), lr=lr)\n",
    "\n",
    "    student.to(device).train()\n",
    "    teacher.to(device).eval()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(train_loader, desc=\"[Student Distill]\", leave=False)\n",
    "        for batch in loop:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                t_logits = teacher(\n",
    "                    input_ids, attention_mask=attention_mask\n",
    "                ).logits / temperature\n",
    "                t_soft = torch.softmax(t_logits, dim=-1)\n",
    "\n",
    "            s_logits = student(input_ids, attention_mask)\n",
    "            s_logits_temp = s_logits / temperature\n",
    "\n",
    "            loss_hard = ce_loss(s_logits, labels)\n",
    "            loss_soft = kl_loss(\n",
    "                torch.log_softmax(s_logits_temp, dim=-1),\n",
    "                t_soft\n",
    "            ) * (temperature ** 2)\n",
    "\n",
    "            loss = alpha * loss_hard + (1 - alpha) * loss_soft\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loop.set_postfix(distill_loss=loss.item())\n",
    "    return student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd6d0398",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439,
     "referenced_widgets": [
      "ab01fbb622bb42f595642a753b6e1847",
      "209fca1a03c94954a7335e2330da68ee",
      "88647eb7f3b14de4a646481fe71284d5",
      "1ecdede5194a4b29af7ce19b1e13558c",
      "7361957f6c4d47cab96cb614a6b2ef3a",
      "306ff166104d4a4583e3dd30c02502ab",
      "33779ce3d86747e3b83187306cf5ea88",
      "b35f0adc47f645dbaf54f637aee51341",
      "c9fa22e4917b4f16a72e53fa228c6c73",
      "bda656bad28a4d3698ff2ede1d344457",
      "4ea9261fc86847e6a042a8b94764462a",
      "478881f3cebf4b4fa979e266ffec4093",
      "4a40b2d7fff34f74bba6789c71b9d3a7",
      "14bd273351974eab97c716bb0d42ac40",
      "82c8434640264686b15edd0d523b84db",
      "ea03ab31f89f49cf953fe91ed25b2ea4",
      "abb603863cc14f5184e8745f7c5c0a83",
      "22bef25117ad4bc6b16ff2b806166ef0",
      "886fcd7e2fe14e62b2f894f3d68aff6e",
      "93526303ca2e440ca3ef279dfc318d77",
      "b1e419e3af5049b4aec5882075e471f8",
      "3b053e44503e48c59268dd5b7d574cf9"
     ]
    },
    "id": "dd6d0398",
    "outputId": "99d177ac-792f-4238-9eea-ed66cbe1b6c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab01fbb622bb42f595642a753b6e1847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Teacher] Epoch 1:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478881f3cebf4b4fa979e266ffec4093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Student Distill]:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-934e860131aa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mteacher_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mstudent_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mretention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_acc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mteacher_acc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mteacher_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-68f26a8104c0>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m             }\n\u001b[1;32m     13\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             metric.add_batch(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "def train_teacher(model, train_loader, device, lr=2e-5, epochs=1):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = get_scheduler(\n",
    "        \"linear\", optimizer=optimizer,\n",
    "        num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    model.to(device).train()\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(train_loader, desc=f\"[Teacher] Epoch {epoch+1}\", leave=False)\n",
    "        for batch in loop:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            # Separate labels from inputs\n",
    "            labels = batch.pop(\"label\")\n",
    "            # Pass only the expected inputs to the model\n",
    "            loss = model(**batch, labels=labels).loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "    return model\n",
    "\n",
    "# Main orchestration\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    train_loader, test_loader, tokenizer = load_data()\n",
    "\n",
    "    teacher = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=2\n",
    "    )\n",
    "    teacher = train_teacher(teacher, train_loader, device)\n",
    "\n",
    "    student = StudentModel(vocab_size=tokenizer.vocab_size)\n",
    "    student = train_student(student, teacher, train_loader, device)\n",
    "\n",
    "    teacher_acc = evaluate_model(teacher, test_loader, device)\n",
    "    student_acc = evaluate_model(student, test_loader, device)\n",
    "    retention = student_acc / teacher_acc * 100 if teacher_acc > 0 else 0\n",
    "\n",
    "    print(f\"Teacher Accuracy : {teacher_acc:.4f}\")\n",
    "    print(f\"Student Accuracy : {student_acc:.4f}\")\n",
    "    print(f\"Retention        : {retention:.1f}% of teacher\")\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    teacher.save_pretrained(\"models/teacher_bert\")\n",
    "    torch.save(student.state_dict(), \"models/student.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "XUtlARBcQckb",
   "metadata": {
    "id": "XUtlARBcQckb"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    model.to(device).eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # move data\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # get outputs: may be a ModelOutput or a Tensor\n",
    "            try:\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            except TypeError:\n",
    "                # fallback for student: positional args\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            # extract logits\n",
    "            if hasattr(outputs, \"logits\"):\n",
    "                logits = outputs.logits\n",
    "            else:\n",
    "                logits = outputs\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            metric.add_batch(\n",
    "                predictions=preds.cpu(),\n",
    "                references=labels.cpu()\n",
    "            )\n",
    "\n",
    "    return metric.compute()[\"accuracy\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "Xlyo8Sf3bBWl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xlyo8Sf3bBWl",
    "outputId": "d0dcd3c3-7146-4f7b-89ab-f7257f5ee1b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Accuracy : 0.9104\n",
      "Student Accuracy : 0.8284\n"
     ]
    }
   ],
   "source": [
    "# after training both...\n",
    "teacher_acc = evaluate_model(teacher, test_loader, device)\n",
    "student_acc = evaluate_model(student, test_loader, device)\n",
    "\n",
    "print(f\"Teacher Accuracy : {teacher_acc:.4f}\")\n",
    "print(f\"Student Accuracy : {student_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
